This has been generated by the StormCrawler Maven Archetype as a starting point for building your own crawler with Elasticsearch as a backend.
Have a look at the code and resources and modify them to your heart's content. 

First generate an uberjar:

``` sh
mvn clean package
```

then with Elasticsearch running, run `./ES_IndexInit.sh` to define the indices used by StormCrawler.

The first step consists in injecting URLs into the status index, which is done by creating a file _seeds.txt_ in the current directory and populating it with the URLs 
to be used as a starting point, e.g. 

`echo "http://stormcrawler.net/" > seeds.txt`

``` sh
storm jar target/${artifactId}-${version}.jar ${package}.CrawlTopology -conf crawler-conf.yaml -local
```

This will run the topology in local mode. Simply remove the '-local' to run the topology in distributed mode.

You can also use Flux to do the same:

``` sh
storm jar target/${artifactId}-${version}.jar  org.apache.storm.flux.Flux --local crawler.flux --sleep 86400000
```

Note that in local mode, Flux uses a default TTL for the topology of 60 secs. The command above runs the topology for 24 hours.

It is best to run the topology with `--remote` to benefit from the Storm UI and logging. In that case, the topology runs continuously, as intended. 
